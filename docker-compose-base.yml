services:
  redis:
    image: redis:7-alpine
    container_name: shared_redis
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    volumes:
      - redis_data:/data
    ports:
      - "127.0.0.1:6379:6379"

  minio:
    image: docker.io/bitnami/minio:2024
    ports:
      - '9000:9000'
      - '9001:9001'
    volumes:
      - 'minio_data:/data'
    environment:
      - MINIO_ROOT_USER=$AWS_ACCESS_KEY_ID
      - MINIO_ROOT_PASSWORD=$AWS_SECRET_ACCESS_KEY
      - MINIO_DEFAULT_BUCKETS=$AWS_STORAGE_BUCKET_NAME
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  meilisearch:
    image: getmeili/meilisearch:v1.13.3
    container_name: meilisearch
#    environment:
#      - MEILI_MASTER_KEY=${MEILI_MASTER_KEY}
    ports:
      - "7700:7700"
    volumes:
      - ./meili_data:/meili_data
    restart: unless-stopped

  postgres:
    image: 'postgis/postgis:15-3.3'  # newer versions dont support M* architecture yet
    command: postgres -c max_connections=270 -c shared_buffers=1024MB -p 5432
    environment:
      SECRET_KEY: ${SECRET_KEY}
      POSTGRES_USER: ${DB_USER}
      PGUSER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_PORT: 5432
      POSTGRES_DB: ${DB_NAME}
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -p 5432" ]
      interval: 10s
      timeout: 5s
      retries: 5
    ports:
      - "127.0.0.1:5432:5432"
    volumes:
      - type: tmpfs
        target: /var/lib/postgresql/data

  clamav:
    image: mkodockx/docker-clamav:alpine
    container_name: clamav
    restart: unless-stopped
    ports:
      - "3310:3310"  # clamd TCP socket
    mem_limit: 2g
    memswap_limit: 3g
    healthcheck:
      test: [ "CMD-SHELL", "clamdscan --version || exit 1" ]
      interval: 1m
      timeout: 10s
      retries: 3
    environment:
      - CLAMD_CONF_MaxFileSize=25M  # bump if needed
      - CLAMD_CONF_StreamMaxLength=25M
    volumes:
      - clamav_defs:/var/lib/clamav  # persist virus definitions

  # Observability Stack

  loki:
    image: grafana/loki:3.0.0
    container_name: loki
    restart: unless-stopped
    ports:
      - "127.0.0.1:3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./observability/loki-config.yaml:/etc/loki/local-config.yaml
      - loki_data:/loki
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3100/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
    mem_limit: 1g
    cpus: 1.0

  tempo:
    image: grafana/tempo:2.5.0
    container_name: tempo
    restart: unless-stopped
    user: root
    ports:
      - "127.0.0.1:3200:3200"  # Tempo HTTP
      - "127.0.0.1:4317:4317"  # OTLP gRPC
      - "127.0.0.1:4318:4318"  # OTLP HTTP
    command: -config.file=/etc/tempo/tempo.yaml
    volumes:
      - ./observability/tempo-config.yaml:/etc/tempo/tempo.yaml
      - tempo_data:/var/tempo
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3200/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
    mem_limit: 1g
    cpus: 1.0

  prometheus:
    image: prom/prometheus:v2.53.0
    container_name: prometheus
    restart: unless-stopped
    ports:
      - "127.0.0.1:9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    volumes:
      - ./observability/prometheus-config.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 5
    mem_limit: 2g
    cpus: 2.0
    extra_hosts:
      - "host.docker.internal:host-gateway"

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    container_name: postgres-exporter
    restart: unless-stopped
    ports:
      - "127.0.0.1:9187:9187"
    environment:
      DATA_SOURCE_NAME: "postgresql://${DB_USER}:${DB_PASSWORD}@postgres:5432/${DB_NAME}?sslmode=disable"
    depends_on:
      postgres:
        condition: service_healthy
    mem_limit: 256m
    cpus: 0.5

  redis-exporter:
    image: oliver006/redis_exporter:v1.58.0
    container_name: redis-exporter
    restart: unless-stopped
    ports:
      - "127.0.0.1:9121:9121"
    environment:
      REDIS_ADDR: "redis:6379"
    depends_on:
      redis:
        condition: service_healthy
    mem_limit: 256m
    cpus: 0.5

  # Pyroscope - Continuous profiling backend
  # NOTE: Grafana Alloy (below) uses eBPF to profile containers and sends data here
  pyroscope:
    image: grafana/pyroscope:1.9.0
    container_name: pyroscope
    restart: unless-stopped
    ports:
      - "127.0.0.1:4040:4040"  # Pyroscope UI
    volumes:
      - pyroscope_data:/var/lib/pyroscope
    environment:
      - PYROSCOPE_LOG_LEVEL=info
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:4040/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
    mem_limit: 2g
    cpus: 2.0

  # Grafana Alloy - eBPF-based profiling agent
  # NOTE: Does NOT work on Docker for Mac (eBPF not supported)
  # Works on Linux only (production environments)
  alloy:
    image: grafana/alloy:latest
    container_name: alloy
    restart: unless-stopped
    privileged: true  # Required for eBPF
    pid: host  # Access host processes
    ports:
      - "127.0.0.1:12345:12345"  # Alloy UI
    volumes:
      - ./observability/alloy-config.alloy:/etc/alloy/config.alloy:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker socket for container discovery
      - alloy_data:/var/lib/alloy
    environment:
      - DEPLOYMENT_ENVIRONMENT=${DEPLOYMENT_ENVIRONMENT:-development}
      - HOSTNAME=${HOSTNAME:-localhost}
    command:
      - run
      - /etc/alloy/config.alloy
      - --storage.path=/var/lib/alloy
      - --server.http.listen-addr=0.0.0.0:12345
    depends_on:
      pyroscope:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:12345/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
    mem_limit: 512m
    cpus: 1.0

  grafana:
    image: grafana/grafana:11.1.0
    container_name: grafana
    restart: unless-stopped
    ports:
      - "127.0.0.1:3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - ./observability/grafana-datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml
      - ./observability/grafana-dashboards.yaml:/etc/grafana/provisioning/dashboards/dashboards.yaml
      - ./observability/dashboards:/var/lib/grafana/dashboards
      - grafana_data:/var/lib/grafana
    depends_on:
      loki:
        condition: service_healthy
      tempo:
        condition: service_healthy
      prometheus:
        condition: service_healthy
      pyroscope:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    mem_limit: 512m
    cpus: 0.5


volumes:
  minio_data:
    driver: local
  clamav_defs:
  redis_data:
  loki_data:
  tempo_data:
  prometheus_data:
  pyroscope_data:
  alloy_data:
  grafana_data: